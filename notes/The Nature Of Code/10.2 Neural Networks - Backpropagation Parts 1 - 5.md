# 10.12 Neural Networks: Backpropagation

## Introduction
After the forward propagation/feedforward algorithm part of training a neural network, comes the actual training, backpropagation. It compares its output against the correct output and goes backwards, from the output to the input layer, adjusting the values of weight $w$ and biases $b$ to try and produce a better output next iteration.

\[
    Output = \begin{bmatrix}
    o_1 \\ o_2 \\ o_3
    \end{bmatrix}
    Desired = \begin{bmatrix}
    d_1 \\ d_2 \\ d_3
    \end{bmatrix}
    Error = \begin{bmatrix}
    d_1 - o_1 \\ d_2 - o_2 \\ d_3 - o_3
    \end{bmatrix}
\]

As all layers within a neural network are interconnected, changes in a layer also affect weights and biases in the previous layer.

## A Simple Example
For a simple neural network:

\[
    I_1 \searrow \hspace{2.5cm} \\
    I_2 \rightarrow H_1 \rightarrow O_1 \rightarrow 0.7 \\
    I_3 \nearrow \hspace{2.5cm}
\]

Ignoring biases, if the weights of the inputs to the the firsst hidden layer are $0.1$, $0.4$ and $0.5$ for weights $w_1$, $w_2$ and $w_3$, we can state that $w_1$ is $10\%$ responsible for the error of the output, etc. This gives us a proportion for how much to nudge the weights in whichever direction to move the output closer to the desired result.

To get the error of eacg weight, get it's output neuron error and get its contribution to it:
\[
    \bm{E_{h1} = \frac{w_1}{w_1 + w_2 + \dots + w_n} \cdot E_{output}} \\
    \vdots \\
    \bm{E_{hn} = \frac{w_n}{w_1 + w_2 + \dots + w_n} \cdot E_{output}}
\]

However, for more complex networks with more nodes per layer and more layers, in a dense neural network, each layer is <b>densely</b> connected to the next layer along with the application of its activation function.

For 2 densely connected layers with 2 nodes each:
\[
    \bm{E_{h1} = \frac{w_{11}}{w_{11} + w_{12}} \cdot E_1 + \frac{w_{21}}{w_{21} + w_{22}} \cdot E_2}\hspace{0.3cm} \\
    \vdots \\
    \bm{E_{hn} = \frac{w_{n1}}{w_{n1} + w_{n2}} \cdot E_n + \frac{w_{n1}}{w_{n1} + w_{n2}} \cdot E_{n + 1}} \\
\]
So on and so forth where the error of each node is the proportion of the error of its output nodes given by its weight as a proportion of all other weights between the same pair of layers.

## Simplification
In the above hidden layer error function, we see that the proportions are all derived the same way, the weight $w_ij$ divided by the sum of weights. Since $w_{ij}$ is directly proportional to $\bm{\frac{w_{ij}}{\sum{w}}}$, the above equation can be rewritten as:
\[
    \bm{E_{h1} = w_{11} \cdot E_1 + w_{21} \cdot E_2}\hspace{0.3cm} \\
    \vdots \\
    \bm{E_{hn} = w_{n1} \cdot E_n + w_{n1} \cdot E_{n + 1}} \\
\]

## Hidden Error 
The error of the hidden layers can be defined as the equations above, but it'll be much more convenient if expressed as a matrix operation like so:
\[
\bm{\begin{bmatrix}
    e_{h1} \\
    e_{h2} \\
    \vdots \\
    e_{hn}
\end{bmatrix}} =
\bm{\begin{bmatrix}
    w_{11} & w_{12} & \dots & w_{1n} \\
    w_{21} & w_{22} & \dots & w_{2n} \\
    \vdots & \vdots & & \vdots & \\
    w_{m1} & w_{m_2} & \dots & w_{mn} 
\end{bmatrix}}^{\bm{T}}
\bm{\begin{bmatrix}
    e_{1} \\
    e_{2} \\
    \vdots \\
    e_{n}
\end{bmatrix}}
\]

## Implementation
Following the `NeuralNetwork.feed_forward` method written in <b>10.1 Neural Networks - Feedforward Algorithm Parts 1 - 2</b>:
```py
class NeuralNetwork:
    def __init__(...): ...
    def feed_forward(...): ...
    
    def train(inputs, targets, learning_rate: float):
        # calculate error of outputs, ERROR = TARGETS - OUTPUTS
        output = self.feed_forward(inputs)
        output_errors = Matrix.sub(targets, outputs)

        # get hidden layer errors
        weights_ho_t = Matrix.transpose(self._weights_ho)
        hidden_errors = Matrix.multiply(weights_ho_t, output_errors)

        # one step of backpropagation
        gradients = Matrix.map(output, lambda x: x * (1 - x)) # deriv of sigmoid
        gradients.multiply(output_errors)
        gradients.multiply(learning_rate)
        
        # update weights and biases
        hidden_T = Matrix.transpose(hidden_errors)
        weight_ho_deltas = Matrix.multiply(gradients, hidden_T)
        self._weights_ho.add(weight_ho_deltas)
        self._bias_o.add(gradients)

        # calculate hidden layer gradients
        hidden_gradient = Matrix.map(hidden, dsigmoid)
        hidden_gradient.multiply(hidden_errors)
        hidden_gradient.multiply(learning_rate)

        # other step of backpropagation (hidden to input)
        inputs_T = Matrix.transpose(inputs)
        weight_ih_deltas = Matrix.multiply(hidden_gradients, inputs_T)
        self._weights_ih.add(weight_ih_deltas)
        self._bias_h.add(hidden_gradient)
```
Note that the above implementation for this part only updates the values for the weights $\bm{w_{ij}}$ and not the biases $\bm{b_i}$, continued in the next part.

## Extras
### Softmax Derivative
The softmax activation function is special in that it is exclusively used in the output layer for multi-class classification neural networks, an "array-wise" operation over the output nodes. It's equation can be expressed as:
\[\bm{\frac{e^{w_1}}{e^{w_1} + e^{w_2} + \dots + e^{w_n}}}\]
Or:
\[\bm{\frac{e^{z_i}}{\sum^K_{j=1}e^{z_j}}}\]

Since its function output value depends on the rest of the outputs, its derivative also does.

Using the `iris.csv` dataset as an example, a set of 4 datapoints to predict the class among e types of flowers, the softmax function can be expressed as:
\[\bm{Softmax_{Setosa}(outputs) = \frac{e^{Setosa}}{e^{Setosa} + e^{Versicolor} + e^{Virginica}} = p_{Setosa}}\]

Using the **quotient rule**: 
\[\bm{
    h(x) = \frac{f(x)}{g(x)} \hspace{1cm}
    h'(x) = \frac{f'(x) \cdot g(x) - f(x) \cdot g'(x)}{g(x)^2}
}\]

We can get the derivative of the probability $p^{Setosa}$ w.r.t (with respect to) its raw input value to the softmax function:
\[\bm{
    \frac{d \hspace{0.1cm} p_{Setosa}}{d \hspace{0.1cm} Raw_{Setosa}} = \frac{(e^{Setosa} \cdot (e^{Setosa} + e^{Versicolor} + e^{Virginica})) - (e^{Setosa} \cdot e^{Setosa})}{(e^{Setosa} + e^{Versicolor} + e^{Virginica})^2}  
}\]
\[\bm{
    \frac{d \hspace{0.1cm} p_{Set}}{d \hspace{0.1cm} Raw_{Set}} = \frac{(e^{Set} \cdot (e^{Set} + e^{Ver} + e^{Vir})) - (e^{Set} \cdot e^{Set})}{(e^{Set} + e^{Ver} + e^{Vir})^2}  
}\]
\[\bm{
    \frac{d \hspace{0.1cm} p_{Set}}{d \hspace{0.1cm} Raw_{Set}} = \frac{e^{Set}[(e^{Set} + e^{Ver} + e^{Vir}) - e^{Set}]}{(e^{Set} + e^{Ver} + e^{Vir})^2}
}\]
\[\bm{
    \frac{d \hspace{0.1cm} p_{Set}}{d \hspace{0.1cm} Raw_{Set}} = \frac{e^{Set}}{(e^{Set} + e^{Ver} + e^{Vir})} \cdot \bigg[ \frac{(e^{Set} + e^{Ver} + e^{Vir})}{(e^{Set} + e^{Ver} + e^{Vir})} - \frac{e^{Set}}{(e^{Set} + e^{Ver} + e^{Vir})} \bigg]
}\]
\[\bm{
    \frac{d \hspace{0.1cm} p_{Set}}{d \hspace{0.1cm} Raw_{Set}} = p_{Set} \cdot (1 - p_{Set})
}\]

Since the raw output values for `Virginica` and `Versicolor` affect the softmax function output value for `Setosa`, differentiation w.r.t the other 2 is done. This part is easier, since for partial differentiation, any variables in not in the "numerator" of the derivation $\frac{dy}{dx}$ is set as $0$.
\[\bm{
    \frac{d \hspace{0.1cm} p_{Ver}}{d \hspace{0.1cm} Raw_{Ver}} = p_{Ver} \cdot (1 - p_{Ver})
}\]
\[\bm{
    \frac{d \hspace{0.1cm} p_{Set}}{d \hspace{0.1cm} Raw_{Ver}} = \frac{0 - e^{Ver} \cdot e^{Set}}{(e^{Set} + e^{Ver} + e^{Vir})^2} = \frac{-e^{Ver}}{(e^{Set} + e^{Ver} + e^{Vir})} \cdot \frac{e^{Set}}{(e^{Set} + e^{Ver} + e^{Vir})}
}\]
\[\bm{
    \frac{d \hspace{0.1cm} p_{Set}}{d \hspace{0.1cm} Raw_{Ver}} = -p_{Ver} \cdot p_{Set}
}\]
