# 10.12 Neural Networks: Backpropagation

## Introduction
After the forward propagation/feedforward algorithm part of training a neural network, comes the actual training, backpropagation. It compares its output against the correct output and goes backwards, from the output to the input layer, adjusting the values of weight $w$ and biases $b$ to try and produce a better output next iteration.

\[
    Output = \begin{bmatrix}
    o_1 \\ o_2 \\ o_3
    \end{bmatrix}
    Desired = \begin{bmatrix}
    d_1 \\ d_2 \\ d_3
    \end{bmatrix}
    Error = \begin{bmatrix}
    d_1 - o_1 \\ d_2 - o_2 \\ d_3 - o_3
    \end{bmatrix}
\]

As all layers within a neural network are interconnected, changes in a layer also affect weights and biases in the previous layer.

## A Simple Example
For a simple neural network:

\[
    I_1 \searrow \hspace{2.5cm} \\
    I_2 \rightarrow H_1 \rightarrow O_1 \rightarrow 0.7 \\
    I_3 \nearrow \hspace{2.5cm}
\]

Ignoring biases, if the weights of the inputs to the the firsst hidden layer are $0.1$, $0.4$ and $0.5$ for weights $w_1$, $w_2$ and $w_3$, we can state that $w_1$ is $10\%$ responsible for the error of the output, etc. This gives us a proportion for how much to nudge the weights in whichever direction to move the output closer to the desired result.

To get the error of eacg weight, get it's output neuron error and get its contribution to it:
\[
    \bm{E_{h1} = \frac{w_1}{w_1 + w_2 + \dots + w_n} \cdot E_{output}} \\
    \vdots \\
    \bm{E_{hn} = \frac{w_n}{w_1 + w_2 + \dots + w_n} \cdot E_{output}}
\]

However, for more complex networks with more nodes per layer and more layers, in a dense neural network, each layer is <b>densely</b> connected to the next layer along with the application of its activation function.

For 2 densely connected layers with 2 nodes each:
\[
    \bm{E_{h1} = \frac{w_{11}}{w_{11} + w_{12}} \cdot E_1 + \frac{w_{21}}{w_{21} + w_{22}} \cdot E_2}\hspace{0.3cm} \\
    \vdots \\
    \bm{E_{hn} = \frac{w_{n1}}{w_{n1} + w_{n2}} \cdot E_n + \frac{w_{n1}}{w_{n1} + w_{n2}} \cdot E_{n + 1}} \\
\]
So on and so forth where the error of each node is the proportion of the error of its output nodes given by its weight as a proportion of all other weights between the same pair of layers.

## Simplification
In the above hidden layer error function, we see that the proportions are all derived the same way, the weight $w_ij$ divided by the sum of weights. Since $w_{ij}$ is directly proportional to $\bm{\frac{w_{ij}}{\sum{w}}}$, the above equation can be rewritten as:
\[
    \bm{E_{h1} = w_{11} \cdot E_1 + w_{21} \cdot E_2}\hspace{0.3cm} \\
    \vdots \\
    \bm{E_{hn} = w_{n1} \cdot E_n + w_{n1} \cdot E_{n + 1}} \\
\]

## Hidden Error 
The error of the hidden layers can be defined as the equations above, but it'll be much more convenient if expressed as a matrix operation like so:
\[
\bm{\begin{bmatrix}
    e_{h1} \\
    e_{h2} \\
    \vdots \\
    e_{hn}
\end{bmatrix}} =
\bm{\begin{bmatrix}
    w_{11} & w_{12} & \dots & w_{1n} \\
    w_{21} & w_{22} & \dots & w_{2n} \\
    \vdots & \vdots & & \vdots & \\
    w_{m1} & w_{m_2} & \dots & w_{mn} 
\end{bmatrix}}^{\bm{T}}
\bm{\begin{bmatrix}
    e_{1} \\
    e_{2} \\
    \vdots \\
    e_{n}
\end{bmatrix}}
\]

## Implementation
Following the `NeuralNetwork.feed_forward` method written in <b>10.1 Neural Networks - Feedforward Algorithm Parts 1 - 2</b>:
```py
class NeuralNetwork:
    def __init__(...): ...
    def feed_forward(...): ...
    
    def train(inputs, targets, learning_rate: float):
        # calculate error of outputs, ERROR = TARGETS - OUTPUTS
        output = self.feed_forward(inputs)
        output_errors = Matrix.sub(targets, outputs)

        # get hidden layer errors
        weights_ho_t = Matrix.transpose(self._weights_ho)
        hidden_errors = Matrix.multiply(weights_ho_t, output_errors)

        # one step of backpropagation
        gradients = Matrix.map(output, lambda x: x * (1 - x)) # deriv of sigmoid
        output.multiply(output_errors)
        output.multiply(learning_rate)
        
        hidden_T = Matrix.transpose(hidden_errors)
        weight_ho_deltas = Matrix.multiply(gradients, hidden_T)
        self._weights_ho.add(weight_ho_deltas)

        # other step of backpropagation (hidden to input)
        
```
